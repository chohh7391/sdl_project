import os
os.environ["UNSLOTH_DISABLE_STATISTICS"] = "1"
import torch
import time
from unsloth import FastLanguageModel


class ToolLLM:
    def __init__(self):
        print("â³ [ToolLLM] ëª¨ë¸ì„ ë¡œë”© ì¤‘ì…ë‹ˆë‹¤. ì ì‹œë§Œ ê¸°ë‹¤ë ¤ì£¼ì„¸ìš”... (ì•½ 10~20ì´ˆ ì†Œìš”)")
        self.llama_path = os.path.join(os.path.dirname(__file__), "..", "..", "..", "llama")
        self.model_path = os.path.join(self.llama_path, "model", "checkpoint", "tool_move", "checkpoint")
        self.model, self.tokenizer = FastLanguageModel.from_pretrained(
            model_name=self.model_path,
            max_seq_length=256,
            load_in_4bit=True,
        )
        self.model.eval()
        FastLanguageModel.for_inference(self.model)

        self.prompt_template = """You are a tool selection system for laboratory automation.

Given a single XDL step and a space constraint flag, output:
1) the main tool for the XDL step
2) whether a Move is needed (True or False)
3) the tool required for the Move (or None)

Rules:
- Output EXACTLY three tokens separated by commas.
- The order MUST be: main_tool, need_move, move_tool
- Allowed tool tokens: dh3, ag95, vgc10, None
- Allowed boolean tokens: True, False
- Do NOT add explanations or extra text.

XDL Step:
{xdl}

is_space_constrained:
{is_space_constrained}

Output:
"""
        self.valid_tools = {"dh3", "ag95", "vgc10", "None"}
        self.valid_booleans = {"True", "False"}

        # ì´ˆê¸° ì›œì—… (ì‹¤ì œ ì¶”ë¡  ì‹œ ì†ë„ í–¥ìƒì„ ìœ„í•´ Dummy ë°ì´í„°ë¡œ 2íšŒ ì‹¤í–‰)
        print("ğŸ”¥ [ToolLLM] ëª¨ë¸ ì›œì—… ì¤‘...")
        dummy_prompt = self.prompt_template.format(xdl='<Stir vessel="flask" />', is_space_constrained=False)
        inputs = self.tokenizer(dummy_prompt, return_tensors="pt").to("cuda")
        for _ in range(2):
            with torch.no_grad():
                _ = self.model.generate(**inputs, max_new_tokens=15, do_sample=False, temperature=0.0)
        torch.cuda.synchronize()
        print("âœ… [ToolLLM] ëª¨ë¸ ë¡œë”© ë° ì›œì—… ì™„ë£Œ!")

    def predict(self, xdl_step: str, is_space_constrained: bool):
        """
        XDL ìŠ¤í…ê³¼ ê³µê°„ ì œì•½ ì—¬ë¶€ë¥¼ ì…ë ¥ë°›ì•„ í•„ìš”í•œ íˆ´ê³¼ ì‘ì—… ì •ë³´ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
        """

        print('xdl_step:', xdl_step)
        prompt = self.prompt_template.format(
            xdl=xdl_step,
            is_space_constrained=is_space_constrained
        )
        inputs = self.tokenizer(prompt, return_tensors="pt").to("cuda")

        torch.cuda.synchronize()
        start_time = time.perf_counter()

        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=15,
                do_sample=False,
                temperature=0.0,
                repetition_penalty=1.05,
                eos_token_id=self.tokenizer.eos_token_id,
            )

        torch.cuda.synchronize()
        end_time = time.perf_counter()
        
        # ë””ì½”ë”© ë° í”„ë¡¬í”„íŠ¸ ì œê±°
        decoded = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
        generated = decoded[len(prompt):].strip()
        generated = generated.splitlines()[0].strip()  # ì²« ì¤„ë§Œ ì‚¬ìš©

        # ì‰¼í‘œ ê¸°ì¤€ìœ¼ë¡œ íŒŒì‹±
        parts = [p.strip() for p in generated.split(",")]

        # ìœ íš¨ì„± ê²€ì‚¬
        if len(parts) != 3:
            print(f"âš ï¸ [ToolLLM] Unexpected output format: {repr(generated)}")
            return "INVALID", "INVALID", "INVALID"
            
        main_tool, need_move, move_tool = parts

        if main_tool not in self.valid_tools: main_tool = "INVALID"
        if need_move not in self.valid_booleans: need_move = "INVALID"
        if move_tool not in self.valid_tools: move_tool = "INVALID"

        print(f"ğŸ§  [ToolLLM] Inference Time: {end_time - start_time:.4f}s | Result: {main_tool}, {need_move}, {move_tool}")
        return main_tool, need_move, move_tool